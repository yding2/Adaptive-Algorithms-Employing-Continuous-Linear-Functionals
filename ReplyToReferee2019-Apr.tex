\documentclass[letter]{article}
\usepackage{bbm,mathtools,array,longtable,booktabs,graphicx,color,enumitem}

\definecolor{orange}{rgb}{1.0,0.3,0.0}
\definecolor{violet}{rgb}{0.75,0,1}
\newcommand{\frednote}[1]{  {\textcolor{red}  {\mbox{**Fred:} #1}}}
\newcommand{\yuhannote}[1]{ {\textcolor{violet}  {\mbox{**Yuhan:} #1}}}
\newcommand{\tonynote}[1]{ {\textcolor{orange}  {\mbox{**Tony:} #1}}}

\textwidth 6.5in
\textheight 9in
\hoffset -1in
\voffset -1in
\input MACROS.tex
\begin{document}
\title{Response to the Referees}
\maketitle

\noindent We would like to thank you for your careful reviews of our manuscript and your helpful comments. 

\noindent Below, we respond to the points made in your reports.

\subsection*{Report beginning ``The submitted paper \ldots''}

While it is less traditional in numerical analysis to assume that a function belongs to a cone than to a ball, we assert that there are advantages.  We think of the difficulty of a problem not as the computational cost but as the ability for an algorithm to correctly determine the computational cost. Thus, problems inside our cones are easy, but they could be costly depending on the norm of the input function.  

Our example in Figure 1 illustrates what an easy problem is.  When the Fourier coefficients of the input decay steadily, we can easily construct an adaptive algorithm that approximates the solution well, regardless of how large the norm of the input function is.  On the other hand, when the decay is not steady, this is a difficult problem because the adaptive algorithm may be fooled unless the initial sample size is quite large.  Our cones are non-convex, which is consistent with what is known about when adaptive algorithms might be effective \cite{Bak71}.

\begin{enumerate}
    \item The first sentence is Subsection 1.1 has been rephrased.
    \item The sufficient and necessary conditions of the existence of singular value decomposition has been added  to  the last paragraph of Subsection 1.1
    \item \textit{linear functionals are used} changed to 
    \textit{linear functionals that are used}
    \item Inserted \textit{the} before the algorithm
    \item The norm for $\calF$ has been identified.
\end{enumerate}


\subsection*{Report beginning ``This paper proposes \ldots''}

\begin{enumerate}[labelwidth = 10ex]
    \item[$1^{\text{st}}$ \bullet] Replaced
    $\sum\limits_{i =1 }^n$ by $\sum\limits_{i \in \mathbb{N} }$
    \item[$2^{\text{nd}}$ \bullet] Added one paragraph to end of \S 1.1 to foreshadow Example 1 and the numerical example.
    \item[$3^{\text{rd}}$ \bullet] Inserted ``that''
    \item[$4^{\text{th}}$ \bullet] Replaced $A(0,\varepsilon)$ 
    with $A(f,\varepsilon)$
    \item[$5^{\text{th}}$ \bullet] Matched the notation for the sample size to the notation for the corresponding algorithm before Section 3,  for example,
    $\widehat{A} \leftrightarrow \widehat{n}, \widetilde{A} \leftrightarrow \widetilde{n},
    A^* \leftrightarrow n^*$. As after section 3, we used a sequence $\bsn = \{n_0, n_1, \ldots \}.$ 
     \item[$6^{\text{th}}$ \bullet] The norm \emph{does} depend on the $\lambda_i$ because the $u_i$ depend on the $\lambda_i$.  This is clearer now that the norm for $\calF$ has been identified.
    \item[$7^{\text{th}}$ \bullet] Added $\exists \omega >0:$ to the definition
    \item[$8^{\text{th}}$ \bullet] Added for some $p>0$
    \item[$9^{\text{th}}$ \bullet] Changed ``no better than'' to ``no smaller than''
    \item[$10^{\text{th}}$ \bullet] Thanks for the correction. Changed to $1/2 -p$.
     \item[$11^{\text{th}}$ \bullet] Changed ``then'' to ``we have''
      \item[$12^{\text{th}}$ \bullet] Corrected the reference of the formula
      \item[$13^{\text{th}}$ \bullet] Regarding these proofs:
       \begin{itemize}
       \item[$-$] Changed the inequality to 
      $n + 2\le \textup{comp}(\calA(\calC),\varepsilon,\rho) + 2 \le n_j$ and corresponding places to correct the proof.
      \item[$-$] As $n$ is the sample size for a particular function of the optimal algorithm, it is possible 
      $n < \textup{cost}(A^*,\calC,\varepsilon,\rho) =  \textup{comp}(\calA(\calC),\varepsilon,\rho).$ 
           \item[$-$] Removed ``by (21)''
           \item[$-$] Added some derivation to illustrate it.
            \item[$-$] Changed to round brackets
            \item[$-$] Thanks for the suggestion. To make the proof clear, we used a different notation and way to present it.
       \end{itemize}
\end{enumerate}

\subsection*{Report beginning  ``This article studies \ldots''}

Regarding the conditions (6) and (7), we have added some examples and explanation after we introduced these two conditions.

\begin{enumerate}
    \item Changed to ``Hence, the algorithm''
    \item Changed to ``and the algorithm''
    \item Changed to ``If for some $p > 0$ ''
    \item Thanks for the comment. Used a different function.
    \item Thanks for the comment. Added the examples.
    \item Have added the period at the end of the formula.
    \item Thanks for the comment. We added a clarification at the end of Section 2.
    \item Changed to $\lambda_i |\widehat{f}_i|$
    \item Changed the assumption to 
    $\lambda_i |\widehat{f}_i| =\Theta(i^{-p})$
    \item Removed $\forall j  \ge 1$
    \item Changed $\div$ to $/$
    \item There is no $+1$.  The derivation has been checked and expanded.
    \item Corrected the reference of the formula
    \item Erased $\le n_{j^\dagger}+1$ and clarified the argument leading to the inequality.
    \item Thanks for the suggestion. Changed $j^*$ to $j^{\ddagger}$
\end{enumerate}

\bigskip

\noindent With kind regards,\\[0.25cm]
The authors, 

\noindent Y. Ding, F.J. Hickernell,  and Ll.A. Jim\'{e}nez Rugama.

\bibliographystyle{elsarticle-num}
\bibliography{FJH23,FJHown23}

\end{document}