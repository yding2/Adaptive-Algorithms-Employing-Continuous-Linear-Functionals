%
% sample root file for your "contribution" to a contributed volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[graybox,footinfo]{svmult}

\smartqed
\usepackage{mathptmx}       % selects Times Roman as basic font
\usepackage{helvet}         % selects Helvetica as sans-serif font
\usepackage{courier}        % selects Courier as typewriter font
\usepackage{type1cm}        % activate if the above 3 fonts are
% not available on your system
\usepackage{graphicx}       % standard LaTeX graphics tool
% when including figure files

\usepackage{array,colortbl}
\usepackage{amsmath,amsfonts,amssymb,bm} % no amsthm, Springer defines Theorem, Lemma, etc themselves
%\usepackage[mathx]{mathabx}
\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{
	<5> <6> <7> <8> <9> <10>
	<10.95> <12> <14.4> <17.28> <20.74> <24.88>
	mathx10
}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareFontSubstitution{U}{mathx}{m}{n}
\DeclareMathAccent{\widecheck}      {0}{mathx}{"71}



% Note that Springer defines the following already:
%
% \D upright d for differential d
% \I upright i for imaginary unit
% \E upright e for exponential function
% \tens depicts tensors as sans serif upright
% \vec depicts vectors as boldface characters instead of the arrow accent
%
% Additionally we throw in the following common used macro's:
\input{macros}

% Macros below are now inclded in macros.tex from MCQMC 2016 web site
% This spot formerly included macros that are now in macros.tex

% indicator boldface 1:
\DeclareSymbolFont{bbold}{U}{bbold}{m}{n}
\DeclareSymbolFontAlphabet{\mathbbold}{bbold}
%\newcommand{\ind}{\mathbbold{1}}


\usepackage{microtype} % good font tricks

\usepackage[colorlinks=true,linkcolor=black,citecolor=black,urlcolor=black]{hyperref}
\urlstyle{same}
\usepackage{bookmark}
\pdfstringdefDisableCommands{\def\and{, }}
\makeatletter % to avoid hyperref warnings:
\providecommand*{\toclevel@author}{999}
\providecommand*{\toclevel@title}{0}
\makeatother



\usepackage{bbm,mathtools,array,longtable,booktabs,graphicx,color}
%\input FJHDef.tex


\newcommand{\DHJRnorm}[2][{}]{\ensuremath{\left \lVert #2 \right \rVert}_{#1}}
\newcommand{\DHJRbignorm}[2][{}]{\ensuremath{\bigl \lVert #2 \bigr \rVert}_{#1}}
\newcommand{\DHJRabs}[1]{\ensuremath{{\left \lvert #1 \right \rvert}}}


\providecommand{\HickernellFJ}{Hickernell}

\definecolor{orange}{rgb}{1.0,0.3,0.0}
\definecolor{violet}{rgb}{0.75,0,1}
\newcommand{\frednote}[1]{  {\textcolor{red}  {\mbox{**Fred:} #1}}}
\newcommand{\yuhannote}[1]{ {\textcolor{violet}  {\mbox{**Yuhan:} #1}}}
\newcommand{\tonynote}[1]{ {\textcolor{orange}  {\mbox{**Tony:} #1}}}

%\journal{Journal of Complexity}



\begin{document}

\title*{Adaptive Algorithms Employing Continuous Linear Functionals}
\author{Yuhan Ding \and Fred J. Hickernell \and Llu\'{\i}s Antoni Jim\'{e}nez Rugama}
\institute{Yuhan Ding \at MCA 310, Department of Mathematics, Misericordia University,\\ 301 Lake St., Dallas, PA, 18612 \email{}
\and
Fred J. Hickernell \at Center for Interdisciplinary Scientific Computation and \\
Department of Applied Mathematics, Illinois Institute of Technology \\ RE 208, 10 W.\ 32$^{\text{nd}}$ St., Chicago, IL 60616 \email{hickernell@iit.edu}
\and Llu\'{\i}s Antoni Jim\'{e}nez Rugama \at
Department of Applied Mathematics, Illinois Institute of Technology,\\ RE 208, 10 W.\ 32$^{\text{nd}}$ St., Chicago, IL 60616 \email{}}

\maketitle

\abstract{}



\section{Introduction}
Adaptive algorithms determine the design and sample size needed to solve problems to the desired accuracy based on the input function data sampled.  A priori upper bounds on some norm of the input function are not needed, but some underlying assumptions about the input function are required for the adaptive algorithm to succeed.  Here we consider \emph{general linear problems} where a finite number of series coefficients of the input function are used to obtain an approximate solution.  The proposed algorithm produces an approximation with guaranteed accuracy.  Moreover, we demonstrate that  computational cost of our algorithm is essentially no worse than that of the best possible algorithm.  Our adaptive algorithm is defined on a \emph{cone} of input functions.

\subsection{Input and Output Spaces}  Let $\calF$ be a separable Hilbert space of inputs with orthonormal basis $\{u_i\}_{i \in \bbN}$, let $\calG$ be a separable Hilbert space of outputs  with orthonormal basis $\{v_i\}_{i\in \bbN}$, and let their norms be defined as the $\ell^2$-norms of their series coefficients:  
\begin{subequations}\label{probDef}
\begin{gather}
f = \sum_{i\in \bbN} \widehat{f}_i u_i \in \calF, \qquad 
\DHJRnorm[\calF]{f}=\DHJRbignorm[2]{(\widehat{f}_i\big)_{i\in \bbN}}, \\
g = \sum_{i\in \bbN} \widehat{g}_i v_i \in \calG, \qquad \DHJRnorm[\calG]{g}=\DHJRbignorm[2]{(\widehat{g}_i\big)_{i\in \bbN}}.
\end{gather}
Let these two bases be chosen by the singular value decomposition so that the linear solution operator, $S:\calF \to \calG$, satisfies
\begin{gather}
S(u_i) = \lambda_i v_i, \quad i \in \bbN, \qquad S(f) = \sum_{i=1}^n \lambda_i \widehat{f}_i v_i, \\
\lambda_1 \ge \lambda_2 \ge \cdots > 0, \quad \lim_{i \to \infty} \lambda_i = 0, \qquad
\DHJRnorm[\calF \to \calG]{S} := \sup_{f \ne 0} \frac{\DHJRnorm[\calG]{S(f)}}{\DHJRnorm[\calF]{f}} = \lambda_1.
\end{gather}
\end{subequations}
This setting includes, for example, the recovery of functions, derivatives, indefinite integrals, and solutions of linear (partial) differential equations.  We focus on cases where the exact solution generally requires an infinite number of series coefficients, $\widehat{f}_i$, Thus, this setting is not relevant for evaluating the definite integral or any other linear functional, since in those cases $\lambda_2 = \lambda_3 = \cdots = 0$.

\subsection{Solvability}  Let $\calH$ be any subset of $\calF$, and let $\calA(\calH)$ denote the set of deterministic algorithms that successfully approximate the solution operator $S : \calH \to \calG$ to within some error tolerance for all inputs in $\calH$:
\begin{multline} \label{adapErrCrit}
\calA(\calH) : = \left\{ \text{algorithms } A:\calH \times (0,\infty) \rightarrow \calG : 
\right . \\ \left .
\DHJRbignorm[\calG]{S(f) - A(f,\varepsilon)} \le \varepsilon \ \forall f \in \calH, \ \varepsilon > 0 
\right \}.
\end{multline}
Algorithms in $\calA(\calH)$ are allowed to sample adaptively any bounded, linear functionals
of the input function.  They must sample only a finite number of linear functionals for each input function and positive tolerance.  The definition of $\calH$ can be used to construct algorithms in $\calA(\calH)$, but no other a priori knowledge about the input functions is available.  Following \cite{KunEtal19a} we call a problem \emph{solvable} for inputs $\calH$ if $\calA(\calH)$ is non-empty.

Our problem is not solvable for $\calF$, as can be demonstrated by contradiction. For any potential algorithm, we show that there exists some $f \in \calF$, that looks like $0$ to the algorithm, but for which $S(f)$ is far from $S(0) = 0$.   Choose any $A \in \calA(\calF)$ and $\varepsilon > 0$, and let $L_1, \ldots, L_n$ be the linear functionals are used to compute $A(0,\varepsilon)$. Since the output space, $\calG$, is infinite dimensional and $n$ is finite, there exists some nonzero $f \in \calF$ satisfying that $L_1(f) = \cdots = L_n(f) = 0$ with non-zero $S(f)$. This means that $A(cf,\varepsilon) = A(0,\varepsilon)$ for any real $c$, and both of these have approximation error no greater than $\varepsilon$, i.e.,
\begin{align*}
    \varepsilon &\ge \frac 12 \left[ \DHJRnorm[\calG]{S(0) - A(0,\varepsilon)} + \DHJRnorm[\calG]{S(cf) - A(cf,\varepsilon)}\right] \\
    & =  \frac 12 \left[ \DHJRnorm[\calG]{0 - A(0,\varepsilon)} + \DHJRnorm[\calG]{S(cf) - A(0,\varepsilon)}\right] \\
    & \ge  \frac {\DHJRabs{c}\DHJRnorm[\calG]{S(f)}}2  \qquad \text{by the triangle inequality}.
\end{align*}
Since $S(f) \ne 0$, it is impossible for this inequality to hold for all real $c$.  The presumed $A$ does not exist, $\calA(\calF)$ is empty, and our problem is not solvable for $\calF$. However, it is solvable for well-chosen subsets of $\calF$, as will be shown in the sections below.

\subsection{Computational Cost of the Algorithm and Complexity of the Problem} The computational cost of an algorithm $A \in \calA(\calH)$ for $f \in \calH$ and error tolerance $\varepsilon$ is denoted $\textup{cost}(A,f,\varepsilon)$, and is defined as the number of linear functional values required to produce $A(f,\varepsilon)$.  By overloading the notation, we define the cost of algorithms as 
\begin{equation*}
\textup{cost}(A,\calH,\varepsilon) : = \sup \{\textup{cost}(A,f,\varepsilon) : f \in \calH \} \qquad \forall \varepsilon > 0.
\end{equation*}
For unbounded sets, $\calH$, this cost may be infinite.  Therefore, it is also helpful to define the cost of algorithms for input functions in $\calH \cap \calB_{\rho}$, where $\calB_{\rho} : = \{ f \in \calF : \DHJRnorm[\calF]{f} \le \rho \}$ is the ball of radius $\rho$:
\begin{equation*}
\textup{cost}(A,\calH,\varepsilon,\rho) : = \sup \{\textup{cost}(A,f,\varepsilon) : f \in \calH \cap \calB_{\rho} \} \qquad \forall \rho > 0, \ \varepsilon > 0.
\end{equation*}
Finally, we define the complexity of the problem as computational cost of the best algorithm:
\begin{gather*}
\textup{comp}(\calA(\calH),\varepsilon) := \min_{A \in \calA(\calH)} \textup{cost}(A, \calH, \varepsilon), \\
\textup{comp}(\calA(\calH),\varepsilon,\rho) := \min_{A \in \calA(\calH)} \textup{cost}(A, \calH, \varepsilon, \rho).
\end{gather*}
Note that $\textup{comp}(\calA(\calH),\varepsilon,\rho) \ge \textup{comp}(\calA(\calH \cap \calB_{\rho}),\varepsilon)$.  In the former case, the algorithm is unaware  that the input function has norm no greater than $\rho$.  

An optimal algorithm for $\calB_{\rho}$ can be constructed in terms of interpolation based on the first $n$ series coefficients of the input, namely,
\begin{gather}  \label{optAdef}
A_n(f) := \sum_{i=1}^n \lambda_{i} \widehat{f}_{i} v_{i}, \\
\label{errOpt}
\DHJRnorm[\calG]{S(f) - A_n(f)} = \DHJRbignorm[2]{\left(\lambda_{i} \widehat{f}_{i} \right)_{i= n+1}^{\infty}} \le \lambda_{n+1} \DHJRnorm[\calF]{f}.
\end{gather}
Define the non-adaptive, algorithm as
\begin{equation} \label{optBallalg}
\widehat{A}(f,\varepsilon) = A_{n^*}(f), \quad \text{where } n^* = \min\{ n : \lambda_{n+1} \le \varepsilon/\rho \}, \qquad \widehat{A} \in \calA(\calB_{\rho}).
\end{equation}
This algorithm is optimal among algorithms in $\calA(\calB_{\rho})$, i.e.,
\[
\textup{comp}(\calA(\calB_{\rho}),\varepsilon) = \textup{cost}(\widehat{A},\calB_{\rho},\varepsilon) =
\min\{ n : \lambda_{n+1} \le \varepsilon/\rho \}.
\]

To prove this, let $A^*$ be an arbitrary algorithm in $\calA(\calB_{\rho})$, and let $L_1, \ldots, L_N$ be the linear functionals chosen when evaluating this algorithm for the zero function with tolerance $\varepsilon$.  Thus, $A^*(0,\varepsilon)$ is some function of $(L_1(0) , \ldots, L_N(0)) = (0, \ldots, 0)$.  Let $f$ be a linear combination of $u_1, \ldots, u_{n+1}$ with norm $\rho$ satisfying  $L_1(f) = \cdots = L_N(f) = 0$, then $A^*(\pm f) = A^*(0)$, and
\begin{align*}
\varepsilon & \ge \max_{\pm} \DHJRnorm[\calG]{S(\pm f) - A^*(\pm f)} =  \max_{\pm} \DHJRnorm[\calG]{\pm S( f) - A^*(0)} \\
& \ge \frac 12 \left [ \DHJRnorm[\calG]{S( f) - A^*(0)} + \DHJRnorm[\calG]{- S( f) - A^*(0)}\right] \\
& \ge \DHJRnorm[\calG]{S(f)} 
= \DHJRbignorm[2]{\big(\lambda_i\widehat{f}_i\big)_{i=1}^{N+1}} \\
& \ge \lambda_{N+1} \DHJRbignorm[2]{\big(\widehat{f}_i\big)_{i=1}^{N+1}} = \lambda_{N+1} \DHJRnorm[\calF]{f} = \lambda_{N+1} \rho.
\end{align*}
Thus, $\lambda_{N+1} \le \varepsilon/\rho$, and 
\[
\textup{cost}(A^*,\calB_\rho,\varepsilon) \ge \textup{cost}(A^*,0,\varepsilon)  = N \ge \min\{ n : \lambda_{n+1} \le \varepsilon/\rho \} = \textup{cost}(\widetilde{A},\calB_\rho,\varepsilon).
\]
Hence, algorithm $\widehat{A}$ defined in \eqref{optBallalg} is optimal for $\calA(\calB_{\rho})$.

\begin{example} Consider the case of function approximation for periodic functions defined on the [0,1], and algorithm $\widehat{A}$ defined in \eqref{optBallalg}:
	\begin{align*}
	f &= \sum_{k \in \bbZ} \widehat{f}(k) \widehat{u}_{k}  = \sum_{i=1}^n \widehat{f}_i u_i, 
	& S(f) & = \sum_{k \in \bbZ} \widehat{f}(k) \widehat{\lambda}_k \widehat{v}_k = \sum_{i=1}^n \widehat{f}_i \lambda_i \widehat{v}_i, \\
	\widehat{v}_{k} (x) &:= \begin{cases} 1, & k = 0, \\
	\displaystyle \sqrt{2} \sin(2\pi k x), & k > 0, \\
	\displaystyle \sqrt{2} \cos(2\pi k x), & k < 0, \\
	\end{cases} 
	& v_i(x) &= \begin{cases} \widehat{v}_{-i/2}, & i \text{ even},\\
	\widehat{v}_{(i-1)/2}, & i \text{ odd},
	\end{cases} \\
	\widehat{\lambda}_{k} &:= \begin{cases} 1, & k = 0, \\
	\displaystyle \frac{1}{\DHJRabs{k}^r}, & k \ne 0,
	\end{cases}
	& \lambda_i &= \widehat{\lambda}_{\lfloor i/2\rfloor} = \frac{1}{\max(1,\lfloor i/2\rfloor)^r}, \\
	\widehat{u}_{k} &:=  \widehat{\lambda}_k \widehat{v}_k,
	& u_i & = \lambda_i v_i = \begin{cases} \widehat{u}_{-i/2}, & i \text{ even},\\
	\widehat{u}_{(i-1)/2}, & i \text{ odd},
	\end{cases}
	\\
	&&\widehat{f}_i& = \begin{cases} \widehat{f}(-i/2), & i \text{ even},\\
	\widehat{f}((i-1)/2), & i \text{ odd},
	\end{cases}
	\end{align*}
	\begin{align*}
	\textup{comp}(\calA(\calB_{\rho}),\varepsilon) &= \textup{cost}(\widehat{A},\calB_{\rho},\varepsilon) 
	=\min\{ n : \lambda_{n+1} \le \varepsilon/\rho \} \\
	& = \min \left\{ n : \frac{1}{\lfloor (n+1)/2 \rfloor^r} \le \frac{\varepsilon}{\rho} \right\} 
	= 2 \left \lceil \left(\frac{\rho}{\varepsilon} \right)^{1/r} \right \rceil - 1.
	% & \lambda_{n+1} = \frac{1}{\lceil (n+1)/2 \rceil^r}.
	\end{align*}
	Here $\calG=L^2[0,1]$.  The larger the non-negative parameter $r$ is, the faster the $\lambda_i$ tend to 0 as $ i \to 0$, the smaller $\calB_\rho$ is, and the smaller  $\textup{cost}(\widehat{A},\calB_{\rho},\varepsilon)$ is.  For $r = 0$, $\textup{cost}(\widehat{A},\calB_{\rho},\varepsilon) = \infty$.
\end{example}


Our goal is not only to construct algorithms in $\calA(\calH)$ for some $\calH$, but also to determine whether the computational cost of our algorithms are reasonable.  We define an algorithm $A \in \calA(\calH)$ to have \emph{essentially no worse cost} then an algorithm $A^* \in \calA(\calH^*)$ if for some number $\omega$,
\begin{gather}
\textup{cost}(A,\calH,\varepsilon) \le \textup{cost}(A^*,\calH^*,\omega\varepsilon), \qquad \forall \varepsilon > 0, \\
\textup{cost}(A,\calH,\varepsilon,\rho) \le \textup{cost}(A^*,\calH^*,\omega\varepsilon,\rho), \qquad \forall \varepsilon,\rho > 0.
\end{gather}
If the costs of two algorithms are essentially no worse than each other, then we call them essentially the same.  An algorithm whose cost is essentially no worse than the best possible algorithm, is called \emph{essentially optimal}.

To illustrate these concepts, consider a nondecreasing sequence of positive numbers, $ \{\lambda^*_1, \lambda^*_2, \ldots \}$, which converges to $0$, where $\lambda^*_i \ge \lambda_i$ for all $i \in \bbN$.  Also consider an unbounded strictly increasing sequence of nonnegative integers $\bsn = \{n_0, n_1, \ldots \}$.  Define an algorithm $A^*$ analogously to $\widehat{A}$ defined in \eqref{optBallalg}:
\begin{equation} \label{altBallalg}
A^*(f,\varepsilon) = A_{n_{j^\dagger}}(f), \quad \text{where } j^\dagger = \min\{ j : \lambda^*_{n_j+1} \le \varepsilon/\rho \}, \qquad A^* \in \calA(\calB_{\rho}).
\end{equation}
By definition, the cost of algorithm $A^*$ is no better than that of  $\widehat{A}$.  Algorithm $A^*$ may or may not have essentially no worse cost than $\widehat{A}$ depending on the choice of $\bslambda^*$ and $\bsn$.  The table below shows some examples.  Each different case of $A^*$ is labeled as having a cost that is either essentially no worse or essentially worse than that of $\widehat{A}$.
%\[
\everymath{\displaystyle}
\begin{longtable}{>{$}r<{$}>{$}l<{$}>{$}l<{$}>{$}l<{$}>{$}l<{$}}
\toprule \toprule
&\lambda_i = \frac{C}{i^p}
& 
\textup{cost}(\widehat{A},\calB_\rho,\varepsilon) \ge \left ( \frac{C\rho}{\varepsilon} \right)^{1/p} - 1
\\[2ex]
& &
\textup{cost}(\widehat{A},\calB_\rho,\varepsilon) 
<  \left ( \frac{C\rho}{\varepsilon} \right)^{1/p}
\\[2ex]
\midrule
\text{no worse}
&
\lambda^*_i = \frac{C^*}{i^p}, \ n_j = 2^j
&
\textup{cost}(A^*,\calB_\rho,\varepsilon) \le 
2 \left ( \frac{C^*\rho}{\varepsilon} \right)^{1/p}
\\[2ex]
\midrule
\text{worse}
&
\lambda^*_i = \frac{C^*}{i^q}, \ q<p, \ n_j = j
&
\textup{cost}(A^*,\calB_\rho,\varepsilon)  \ge 
\left ( \frac{C^*\rho}{\varepsilon} \right)^{1/q} - 1 
\\[2ex]
\toprule \toprule
&\lambda_i = \frac{C}{p^i}, \ p > 1
& 
\textup{cost}(\widehat{A},\calB_\rho,\varepsilon) \ge \frac{\log (C\rho/\varepsilon)}{\log(p)} - 1
\\[2ex]
&&
\textup{cost}(\widehat{A},\calB_\rho,\varepsilon) < \frac{\log (C\rho/\varepsilon)}{\log(p)}
\\[2ex]
\midrule
\text{no worse}
&
\lambda^*_i = \frac{C^*}{p^i}, \ n_j = 2j
&
\textup{cost}(A^*,\calB_\rho,\varepsilon) < \frac{\log (C^*\rho/\varepsilon)}{\log(p)} + 1
\\[2ex]
\midrule
\text{worse}
&
\lambda^*_i = \frac{C^*}{p^i}, \ n_j = 2^j
&
\textup{cost}(A^*,\calB_\rho,\varepsilon) > 1.999 \frac{\log (C^*\rho/\varepsilon)}{\log(p)} 
\\[1ex]
&& \qquad \qquad \text{for some } \varepsilon
\\
\midrule
\text{worse}
&
\lambda^*_i = \frac{C^*}{i^q}, \ q<p, \ n_j = j
&
\textup{cost}(A^*,\calB_\rho,\varepsilon)  \ge 
\frac{\log (C^*\rho/\varepsilon)}{\log(q)} - 1
\\[2ex]
\toprule \toprule
\end{longtable}
%\]

\subsection{The Case for Adaptive Algorithms}
For bounded sets of input functions, such as balls, non-adaptive algorithms, such as $\widehat{A}$, make sense.  However, it is typically unknown which ball the input function lies in.  Thus, we consider unbounded sets of input functions, whose properties can be inferred from the data we collect.  We use the interpolatory algorithm $A_n$, defined in \eqref{optAdef}, but we propose an alternative way to bound the error other than assuming an a priori bound on $\DHJRnorm[\calF]{f}$.

Adaptive algorithms encountered in practice typically employ heuristic error bounds.  While any algorithm can be fooled, we would like precise necessary conditions for being fooled, or equivalently, sufficient conditions for the algorithm to succeed.  Our adaptive algorithm has such conditions and follows in the vein of adaptive algorithms developed in \cite{HicEtal14a, HIcEtal14b, HicJim16a, JimHic16a}.

Our rigorous, data-driven error bound assumes the input to have steadily---but not necessarily monotonically---decaying series coefficients.  The cone of nice inputs, $\calC$, is defined in Section \ref{sec:cone}.  For such inputs, we construct an adaptive algorithm, $\widetilde{A} \in \calA(\calC)$, in Section \ref{sec:adaptalgo},  where $\widetilde{A}(f,\varepsilon) = A_{n^*}(f)$ for some $n^*$ depending on the input data and the definition of $\calC$.  The number of series coefficients sampled, $n^*$, is adaptively determined so that $\widetilde{A}(f,\varepsilon)$ satisfies the error condition in \eqref{adapErrCrit}.  The computational cost of $\widetilde{A}$ is given in  Theorem \ref{thm:compcost}.  Section \ref{sec:opt} shows that our new algorithm is essentially optimal (see Theorem \ref{thm:CostNoWorse}).  Section \ref{sec:examp} provides an example of our algorithm.  Concluding remarks are give in Section \ref{sec:conc}. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Assuming a Steady Decay of the Series Coefficients of the Solution} \label{sec:cone}

Recall from \eqref{errOpt} that the error of the fixed sample size interpolatory algorithm $A_n$ is $\DHJRnorm[\calG]{S(f) - A_n(f)} = \DHJRbignorm[2]{\bigl(\lambda_{i} \widehat{f}_{i} \bigr)_{i= n+1}^{\infty}}$.  The error depends on the series coefficients not yet observed, so at first glance this error seems to be impossible to bound in terms of observed series coefficients.  

However, we can observe the partial sums 
\begin{equation} \label{sumdef}
\sigma_j(f) :
= \DHJRnorm[2]{ \left(\lambda_{i} \widehat{f}_{i} \right)_{i=n_{j-1}+1}^{n_j}}, \qquad j \in \bbN,
\end{equation}
where $\bsn  = \{n_0, n_1, \ldots\}$ is a strictly increasing, unbounded sequence of non-negative integers.  We define the cone of nice input functions to consist of those functions for which the $\sigma_j(f)$ decay at a given rate with respect to one another:
\begin{align} \label{decayconedef}
\calC &= \left\{ f \in \calF : \sigma_{j+r}(f) \le ab^r \sigma_j (f) \ \forall j,r \in \bbN \right\} \\
\nonumber
& = \left\{ f \in \calF : \sigma_j(f) \le \min_{1 \le r < j}\{ab^r\sigma_{j-r}(f)\} \ \forall j \in \bbN \right\}.
\end{align}
Here, $a$ and $b$ are positive numbers that define the inclusivity of the cone $\calC$ and 
\begin{equation} \label{abcond}
b <1 < a.
\end{equation}
The constant $a$ is an inflation factor, and the constant $b$ defines the general rate of decay of the $\sigma_j(f)$ for $f \in \calC$. We do not expect the series coefficients of the solution $S(f)$, to decay monotonically. However, we expect their partial sums to decay steadily.

From the expression for the error in \eqref{errOpt} and the definition of the cone in  \eqref{decayconedef}, one can now derive a data-driven error bound for $j \in \bbN$:
\begin{align}
\nonumber
\DHJRnorm[\calG]{S(f)-A_{n_j}(f)} &= \DHJRnorm[2]{\left(\lambda_{i} \widehat{f}_{i} \right)_{i = n_j+1}^\infty} = \left\{\sum_{r=1}^\infty \sum_{i=n_{j+r-1}+1}^{n_{j+r}}  \DHJRabs{\lambda_{i}\widehat{f}_{i} }^{2}  \right\}^{1/2}\\
\nonumber
&= \DHJRnorm[2]{ \bigl(\sigma_{j+r}(f)\bigr)_{r=1}^{\infty}} \\
&\le \DHJRnorm[2]{ \bigl(ab^r\sigma_{j}(f)\bigr)_{r=1}^{\infty}}
 = \displaystyle ab \sqrt{\frac{1}{1 - b^2}}\sigma_{j}(f)
 \label{algoineq}
\end{align}
The upper and lower error bounds depend only on the function data and the parameters defining $\calC$.  The error vanishes as $j \to \infty$ because $\sigma_j(f) \le ab^{j-1} \sigma_1(f) \to 0$ as $j \to \infty$.  Moreover, the error of $A_{n_j}(f)$ is asymptotically the no worse than $\sigma_j(f)$, whose rate of decay need not be postulated in advance. Our adaptive algorithm in the Section \ref{sec:adaptalgo} increases $j$ until the right hand side is smaller than the error tolerance.

Consider the choice 
\begin{equation} \label{geonj}
n_j = 2^{j}n_0,
\end{equation}
where the number of terms in the sums, $\sigma_j(f)$, are doubled at each step.  If the series coefficients of the solution decay like $\lambda_{i} \DHJRabs{f_{i}} = \calO(i^{-p})$ for some $p>1$, then it is reasonable to expect that the $\sigma_j(f)$ are bounded above and below as
\begin{equation} \label{algDecJ}
C_{\textup{lo}} (n_02^j)^{1-p} \le \sigma_j(f) \le C_{\textup{up}} (n_02^j)^{1-p}, \quad   j \in \bbN,
\end{equation}
for some constants $C_{\textup{lo}}$ and $C_{\textup{up}}$, unless the series coefficients drop precipitously in magnitude for some $n_{j-1} < i \le n_j$, and then jump back up for larger $i$.  When \eqref{algDecJ} holds, it follows that
\begin{equation*} 
\frac{\sigma_{j+r}(f)}{\sigma_j(f)} \le \frac{C_{\textup{up}} (n_02^{j+r})^{1-p}}{C_{\textup{lo}} (n_02^j)^{1-p}} = \frac{C_{\textup{up}} 2^{r(1-p)}}{C_{\textup{lo}}}\quad   j \in \bbN.
\end{equation*}
Thus, choosing $a \ge C_{\textup{up}}/C_{\textup{lo}}$ and $b \ge 2^{1-p}$ ensures that reasonable inputs $f$ lie inside the cone $\calC$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Adaptive Algorithm} \label{sec:adaptalgo}

Now we introduce our adaptive algorithm, $\widetilde{A} \in \calA(\calC)$, which yields an approximate solution to the problem $S:\calC\rightarrow\calG$ that meets the absolute error tolerance $\varepsilon$.

\begin{algo}\label{algo2}
Given $a$, $b$, the sequence $\bsn$, the cone $\calC$, and the input function $f \in \calC$, and the absolute error tolerance $\varepsilon$, set $j=1$.
\begin{description}
\item[Step 1.] Compute $\sigma_{j}(f)$ as defined in \eqref{sumdef}.
\item[Step 2.] Check whether $j$ is large enough to satisfy the error tolerance, i.e.,
    \begin{equation}\label{covcrit}
          \sigma_{j}(f) \le \frac{\varepsilon\sqrt{1 - b^2}}{ab} .
    \end{equation}
    If this is true, then return $\widetilde{A}(f,\varepsilon) = A_{n_{j}}(f)$, where $A_n$ is defined in \eqref{optAdef}, and terminate the algorithm.
\item[Step 3.] Otherwise, increase $j$ by $1$ and return to Step $1$.
\end{description}
\end{algo}

\begin{theorem}\label{thm:compcost}
The algorithm, $\widetilde{A}$, defined in Algorithm \ref{algo2} lies in $\calA(\calC)$ and has computational cost $\textup{cost}(\widetilde{A},f,\varepsilon)=n_{j^*}$, where $j^*$ is defined implicitly by the inequalities 
\begin{equation} \label{eq:OurAlgjstar}
j^* = \min\left \{ j \in \bbN : \sigma_{j}(f) \le \frac{\varepsilon\sqrt{1 - b^2}}{ab}  \right\}.
\end{equation}
Moreover, $\textup{cost}(\widetilde{A},\varepsilon,\rho) \le n_{j^\dagger}$, where $j^\dagger$ satisfies the following upper bound:
\begin{equation} \label{jdagger}
j^\dagger \le \min \left \{j \in \bbN : \frac{\rho^2}{\varepsilon^2} \le \frac{(1 - b^2)}{a^2b^2} \left[ \sum_{k=1}^{j-1} \frac{b^{2(k-j)}}{a^2\lambda_{n_{k-1}+1}^2} + \frac{1}{\lambda_{n_{j-1}+1}^2}\right]   \right\}.
\end{equation}

\end{theorem}

\begin{proof}
This algorithm terminates for some $j = j^*$ because $\sigma_j(f) \le ab^{j-1} \sigma_{1}(f) \to 0$ as $j \to \infty$. The value of $j^*$ follows directly from this termination criterion in Step 2.  It then follows that the error bound on $A_{n_{j^*}}(f)$ in \eqref{algoineq} is no greater than the error tolerance $\varepsilon$.  So, $\widetilde{A} \in \calA(\calC)$.

For the remainder of the proof consider $\rho$ and $\varepsilon$ fixed.  To derive an upper bound on $n_{j^\dagger} = \textup{cost}(\widetilde{A},\varepsilon,\rho)$ we first note some properties of $\sigma_j(f)$ for all $f \in \calC$:
\begin{multline} \label{normsigineq}
\lambda_{n_j} \DHJRbignorm[2]{\big( \widehat{f}_i \big)_{i=n_{j-1}+1}^{n_j}} \le 
\DHJRbignorm[2]{\big( \lambda_i \widehat{f}_i \big)_{i=n_{j-1}+1}^{n_j}} = \sigma_j(f) 
\\
\le \lambda_{n_{j-1}+1} \DHJRbignorm[2]{\big( \widehat{f}_i \big)_{i=n_{j-1}+1}^{n_j}}.
\end{multline}

A rough upper bound on $j^\dagger$ may be obtained by noting that for any $f \in \calC \cap \calB_\rho$ and for any $j < j^* \le j^\dagger$, it follows from \eqref{eq:OurAlgjstar} and \eqref{normsigineq} that 
\begin{equation*}
\rho \ge \DHJRnorm[\calF]{f} \ge \DHJRbignorm[2]{\big( \widehat{f}_i \big)_{i=n_{j-1}+1}^{n_j}} \ge \frac{\sigma_j(f)}{\lambda_{n_{j-1}+1}} > \frac{\varepsilon\sqrt{1 - b^2}}{ab \lambda_{n_{j-1}+1}}
\end{equation*}
Thus, one upper bound on $j^\dagger$ is the smallest $j$ violating the above inequality:
\begin{equation} \label{jdaggerlast}
j^\dagger \le \min \left \{j \in \bbN :  \lambda_{n_{j-1}+1} \le \frac{\varepsilon\sqrt{1 - b^2}}{ab \rho} \right\}.
\end{equation}

The tighter upper bound in Theorem \ref{thm:compcost} may be obtained by a more careful argument in a similar vein.  
For any $f \in  \calC \cap \calB_\rho$ and for any $j < j^* \le j^\dagger$,
\begin{align*}
\rho^2 &\ge \DHJRnorm[\calF]{f}^2 = \DHJRbignorm[2]{\big( \widehat{f}_i \big)_{i=1}^{\infty}}^2 \\
& \ge \sum_{k=1}^j \DHJRbignorm[2]{\big( \widehat{f}_i \big)_{i=n_{k-1}+1}^{n_k}}^2 \qquad \forall j \ge 1\\
& \ge \sum_{k=1}^j \frac{\sigma_k^2(f)}{\lambda_{n_{k-1}+1}^2} \qquad \text{by \eqref{normsigineq}}\\
& \ge \sum_{k=1}^{j-1} \frac{ b^{2(k-j)}\sigma_j^2(f)}{a^{2}\lambda_{n_{k-1}+1}^2} + \frac{\sigma_j^2(f)}{\lambda_{n_{j-1}+1}^2} \qquad \text{by \eqref{decayconedef}} \\
& = \sigma_j^2(f) \left[ \sum_{k=1}^{j-1} \frac{b^{2(k-j)}}{a^{2} \lambda_{n_{k-1}+1}^2} + \frac{1}{\lambda_{n_{j-1}+1}^2}\right].
\end{align*}
Note that the quantity in the square brackets is an increasing function of $j$ because as $j$ increases, the sum becomes includes more terms and $b^{2(k-j)}$ also increases.

For all $j < j^* \le j^\dagger$ it follows from \eqref{eq:OurAlgjstar} that
\begin{equation*}
\rho^2 > \frac{\varepsilon^2(1 - b^2)}{a^2b^2} \left[ \sum_{k=1}^{j-1} \frac{ b^{2(k-j)}}{a^{2}\lambda_{n_{k-1}+1}^2} + \frac{1}{\lambda_{n_{j-1}+1}^2}\right].
\end{equation*}
Thus, any $j$ that violates the above inequality, must satisfy $j \ge j^\dagger$, establishing \eqref{jdagger}.
\end{proof}

We note in passing that for our adaptive algorithm
\begin{equation*}
 \min \{\textup{cost}(\widetilde{A},f,\varepsilon) : f \in \calC, \ \DHJRnorm[\calF]{f} \ge \rho \} 
 \begin{cases} = n_1, & n_0 > 0, \\
 \le n_2, & n_0 = 0, 
 \end{cases}
 \qquad \forall \rho > 0, \ \varepsilon > 0.
\end{equation*}
This result may be obtained by considering functions where only $\widehat{f}_1$ is nonzero.  For $n_0 > 0$, $\sigma_1(f) = 0$, and for $n_0 = 0$, $\sigma_2(f) = 0$.

The upper bound on $\textup{cost}(\widetilde{A},\rho,\varepsilon)$ in Theorem \ref{thm:compcost}  is a non-decreasing function of $\rho/\varepsilon$, which depends on the behavior of the sequence $\{(\lambda_{n_j})_{j=0}^\infty\}$.  This in turn depends both on the increasing sequence $\bsn$ and on the non-increasing sequence $\{(\lambda_i)_{i=1}^\infty\}$. Consider the  term enclosed in square brackets on the the right hand side of the inequality in \eqref{jdagger}: \begin{equation} \label{keysum}
\sum_{k=1}^{j-1} \frac{b^{2(k-j)}}{a^2\lambda_{n_{k-1}+1}^2} + \frac{1}{\lambda_{n_{j-1}+1}^2}.
\end{equation}
One can imagine that in some cases the first term in the sum dominates, while in other cases the term outside the sum dominates, all depending on how $b^{k-j}/\lambda_{n_{k-1}+1}$ behaves with $k$ and $j$.  These simplifications lead to two simpler, but coarser upper bounds on the cost of $\widetilde{A}$

\begin{corollary} For the algorithm, $\widetilde{A}$, defined in Algorithm \ref{algo2}, then $\textup{cost}(\widetilde{A},\varepsilon,\rho) \le n_{j^\dagger}$, where $j^\dagger$ satisfies the following upper bound:
\begin{equation} \label{jdaggerfirst}
j^\dagger \le \left \lceil \log\left(\frac{\rho a^2\lambda_{n_{0}+1} }{\varepsilon \sqrt{1 - b^2}}\right) \div \log\left(\frac{1}{b}\right) \right \rceil.
\end{equation}
Moreover, if the $\lambda_{n_{j-1}+1}$ decay as quickly as
\begin{equation}
\label{lambdankbd}
\lambda_{n_{j-1}+1} \le \alpha \beta^j,  \quad j \in \bbN,  \qquad \text{for some } \alpha > 0, \ 0 < \beta < 1.
\end{equation}
then $j^\dagger$ also satisfies the following upper bound:
\begin{equation}
\label{jdaggerlastsimple}
j^\dagger \le
\left \lceil \log\left(\frac{\rho a\alpha b }{\varepsilon \sqrt{1 - b^2}}\right) \div \log\left(\frac{1}{\beta}\right) \right \rceil.
\end{equation}
\end{corollary}

\begin{proof}
Ignoring all but the first term in the sum in \eqref{keysum} implies that 
\begin{equation} \label{jsumupperbd}
j^\dagger \le \min \left \{j \in \bbN : \frac{\rho^2}{\varepsilon^2} \le \frac{(1 - b^2)}{a^2b^2} \frac{b^{2(1-j)}}{a^2\lambda_{n_{0}+1}^2}    \right\}.
\end{equation}
This implies \eqref{jdaggerfirst}.

Ignoring all but the term outside the sum leads to the simpler upper bound in \eqref{jdaggerlast}.  If the $\lambda_{n_{j-1}+1}$ decay as assumed in \eqref{lambdankbd} then
\[
j^\dagger \le \min \left \{j \in \bbN :  \alpha \beta^j < \frac{\varepsilon\sqrt{1 - b^2}}{ab \rho} \right\},
\]
which implies \eqref{jdaggerlastsimple}.
\end{proof}

This corollary highlights two limiting factors on the computational cost of our adaptive algorithm, $\widetilde{A}$. When $j$ is large enough to make $\lambda_{n_{j-1}+1}\DHJRnorm[\calF]{f}/\varepsilon$ small enough, $\widetilde{A}(f,\varepsilon)$ stops.  This is statement \eqref{jdaggerlastsimple}, and its precursor, \eqref{jdaggerlast}.  Alternatively, the assumption that the $\sigma_j(f)$ are steadily decreasing, as specified in the definition of $\calC$ in \eqref{decayconedef}, means that $\widetilde{A}(f,\varepsilon)$ also must stop by the time $j$ becomes large enough with respect to $\lambda_{n_0+1}\DHJRnorm[\calF]{f}/\varepsilon$.

Assumption \eqref{lambdankbd} is not very restrictive.  It holds if the $\lambda_i$ decay algebraically and the $n_j$ increase geometrically.  It also holds if the $\lambda_i$ decay geometrically and the $n_j$ increase arithmetically.

The adaptive algorithm $\widetilde{A}$, which does not know an upper bound on $\DHJRnorm[\calF]{f}$ a priori, may cost more than the non-adaptive algorithm $\widehat{A}$, which assumes an upper bound on $\DHJRnorm[\calF]{f}$, but under reasonable assumptions, the extra cost will be small.

\begin{corollary} \label{cor:tAsameCosthA} Suppose that the sequence $\bsn$ is chosen to satisfy
\begin{equation} \label{lambdaDecay}
\lambda_{n_{j+1}+1} \ge c_\lambda \lambda_{n_j+1}, \qquad j \in \bbN, 
\end{equation}
for some positive $c_\lambda$.  Then $\textup{cost}(\widetilde{A},\calC, \varepsilon,\rho)$ is essentially no worse than \linebreak[4]
$\textup{cost}(\widehat{A},\calB_{\rho},\varepsilon)$. 


\end{corollary}

\begin{proof}
Combining the upper bound on $n_{j^\dagger} = \textup{cost}(\widetilde{A},\calC,\varepsilon,\rho)$ in \eqref{jdaggerlast} plus  \eqref{lambdaDecay} above, it follows that
\begin{equation*}
\lambda_{n_{j^\dagger}+1} \ge c_{\lambda}^2 \lambda_{n_{j^\dagger-2}+1} > \frac{\varepsilon c_{\lambda}^2\sqrt{1 - b^2}}{ab \rho} \ge \lambda_{n+1},
\end{equation*}
where $n = \textup{cost}(\widehat{A},\calB_{\rho},\varepsilon c_{\lambda}^2\sqrt{1 - b^2}/ab)$.
Since the $\lambda_i$ are non-increasing,
\begin{equation*}
\textup{cost}(\widetilde{A},\calC,\varepsilon,\rho) = n_{j^\dagger} \le n_{j^\dagger}+1 < n = \textup{cost}(\widehat{A},\calB_{\rho},\varepsilon c_{\lambda}^2\sqrt{1 - b^2}/ab).
\end{equation*}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Essential Optimality of the Adaptive Algorithm} \label{sec:opt}

From Corollary \ref{cor:tAsameCosthA} it is known that $\textup{cost}(\widetilde{A},\calC, \varepsilon,\rho)$ is essentially no worse than
$\textup{cost}(\widehat{A},\calB_{\rho},\varepsilon) = \textup{comp}(\calA(\calB_{\rho}),\varepsilon)$.  We would like to show that $\widetilde{A} \in \calA(\calC)$ is  essentially optimal, i.e., $\textup{cost}(\widetilde{A},\calC, \varepsilon,\rho)$ is essentially no worse than  $\textup{comp}(\calA(\calC),\varepsilon,\rho)$.  However,  $\textup{comp}(\calA(\calC),\varepsilon,\rho)$ may be smaller than $\textup{comp}(\calA(\calB_{\rho}),\varepsilon)$ because $\calC \cap \calB_\rho$ is a strict subset of  $ \calB_\rho$.  This presents a challenge.

A lower bound on $\textup{comp}(\calA(\calC),\varepsilon,\rho)$ is established by constructing fooling functions in $\calC$ with norms no greater than $\rho$.  To obtain a result that can be compared with the cost of our algorithm, we assume that 
\begin{equation} \label{lambdaRatio}
R = \sup_{k \in \bbN} \frac{\lambda_{n_{k-1}}}{\lambda_{n_k}} < \infty.
\end{equation}
This means that the $n_k$ are not too far apart with respect to the decay of $\lambda_i$ as $i \to \infty$.

The following theorem establishes a lower bound on the complexity of our problem for input functions in $\calC$.  The theorem after that shows that the cost of our algorithm as given in Theorem \ref{thm:compcost} is essentially no worse than this lower bound.


\begin{theorem} \label{thm:lowbdcomp}
Under assumption  \eqref{lambdaRatio}, a lower bound on the complexity of the linear problem defined in \eqref{probDef} is
\begin{subequations} \label{compbd}
\begin{align}
 \label{compbdA}
&\textup{comp}(\calA(\calC),\varepsilon,\rho) \ge n_{j^*}, 
\intertext{where}
\label{compbdB}
j^* & = \max \left \{ j \in \bbN : \left[\frac{(a+1)^{2} R^2 }{(a-1)^2} + 1\right] \sum_{k=0}^j \frac{b^{2(k-j)}}{\lambda_{n_{k}}^2}   <
\frac{\rho^2}{\varepsilon^2}
\right \}.
\end{align}
\end{subequations}
\end{theorem}

\begin{proof}
Consider a fixed $\rho$ and $\varepsilon$.  Choose any positive integer $j$ such that $n_j$ exceeds $\textup{comp}(\calA(\calC),\varepsilon,\rho)$.  The proof proceeds by carefully constructing thre  test input functions, $f$ and $f_{\pm}$, lying in $\calC \cap \calB_{\rho}$, which yield the same approximate solution but different true solutions.  This leads to a lower bound on $n_j$, which can be translated into a lower bound on $\textup{comp}(\calA(\calC),\varepsilon,\rho)$. 

The first test function $f \in \calC$ is defined in terms of its series coefficients:
\begin{align}
\nonumber
\widehat{f}_i &:= \begin{cases}
\displaystyle
\frac{c b^{k-j}}{\lambda_{n_{k}}},  & i =  n_{k}, \ k = 1, \ldots, j,
\\
0, & \text{otherwise},
\end{cases}
\\
\label{cdef}
c^2 &:=  \rho^2 \left[ \left(1+\frac{(a-1)^2}{(a+1)^{2} R^2 }\right)\sum_{k=0}^j \frac{b^{2(k-j)}}{\lambda_{n_{k}}^2}\right]^{-1}.
\end{align}
It can be verified that the test function lies both in $\calB_{\rho}$ and in $\calC$:
\begin{align}
\label{testfunnorm}
\DHJRnorm[\calF]{f}^2 &=  c^2\sum_{k=1}^j \frac{ b^{2(k-j)}}{\lambda^2_{n_{k}}} \le \rho^2,
\\
\nonumber
\sigma_k(f) &= \begin{cases}
\displaystyle
c b^{k-j}, & k =1, \ldots, j, \\
0, & \text{otherwise},
\end{cases}
\\
\nonumber
\sigma_{k+r}(f) &= \begin{cases}
\displaystyle 
b^{r} \sigma_k(f) \le a b^r \sigma_k(f), & k+r \le j, \ r \ge 1,
\\
0 \le a b^r \sigma_k(f), & k+r > j, \ r \ge 1.
\end{cases}
\end{align}

Now suppose that $A^* \in \calA(\calC)$ is an optimal algorithm, i.e., $\textup{cost}(A^*,\calC,\varepsilon,\rho) =  \textup{comp}(\calA(\calC),\varepsilon,\rho)$ for all $\varepsilon, \rho > 0$.  For our particular input $f$ defined above, suppose that $A^*(f,\varepsilon)$ samples $L_1(f), \ldots, L_n(f)$ where 
\[
n + 1\le \textup{comp}(\calA(\calC),\varepsilon,\rho) +1 < n_j.
\]  

Let $u$ be a linear combination of $u_1, \cdots, u_{n_j}$, expressed as
\[
u =  \sum_{k=0}^{j}\frac{b^{k-j}u^{(k)}}{\lambda_{n_k}},
\]
where $u^{(0)}$ is a linear combination of $u_{1}, \ldots, u_{n_0}$, and each $u^{(k)}$ is a linear combination of $u_{n_k+1}, \ldots, u_{n_k}$, for $k =1, \ldots, j$.  We constrain $u$ to satisfy:
\begin{equation}\label{uConstraint}
L_1(u) = \cdots = L_n(u) = 0, \qquad \langle u,f \rangle_{\calF} = 0, \qquad 
\max_{0\le k\le j} \DHJRbignorm[\calF]{u^{(k)}} = 1.
\end{equation}
Since $u$ is a linear combination of $n_j >n+1$ basis functions, these constraints can be satisfied.

Let the other two test functions be constructed in terms of $u$ as 
\begin{align}
\label{etadef}
f_\pm & := f \pm \eta u, \qquad \eta : =  \frac {(a-1)c}{(a+1)R}, \\
\nonumber
\DHJRnorm[\calF]{f^*_\pm}^2 & \le \DHJRnorm[\calF]{f}^2 + \DHJRnorm[\calF]{\eta u }^2 \qquad \text{by \eqref{uConstraint}} \\
& \le \sum_{k=1}^j \frac{ b^{2(k-j)}}{\lambda^2_{n_{k}}} \left(c^2+ \eta^2 \DHJRbignorm[\calF]{u^{(k)}}^2\right) + \eta^2 \DHJRbignorm[\calF]{u^{(0)}}^2 \frac{b^{-2j}}{\lambda_{n_0}^2}\\
\nonumber
& \le  \left(c^2+ \eta^2 \right) \sum_{k=0}^j \frac{ b^{2(k-j)}}{\lambda^2_{n_{k}}}  \qquad \text{by \eqref{uConstraint}}\\
&  \le \rho^2,
\end{align} 
so $f_{\pm} \in \calB_{\rho}$.  By design, $A^*(f_\pm,\varepsilon) = A^*(f,\varepsilon)$, which will be used below.

Now we must check that $f_\pm \in \calC$. From the definition in \eqref{sumdef} it follows that for $k = 1, \ldots, j$ and $r \ge 1$,
\begin{equation*}
\sigma_k(f_\pm)  \begin{cases} 
\displaystyle
\le \sigma_k(f) + \sigma_k(\eta u)\le 
c b^{k-j} + \eta\lambda_{n_{k-1}+1}\frac{b^{k-j}\DHJRnorm[\calF]{u^{(k)} } }{\lambda_{n_k}}
\le b^{k-j}\left(c+\eta R \right) 
\\[1ex]
\displaystyle
\ge \sigma_k(f) - \sigma_k(\eta u)\ge 
c b^{k-j} - \eta\lambda_{n_{k-1}+1}\frac{b^{k-j}\DHJRnorm[\calF]{u^{(k)} } }{\lambda_{n_k}}
\ge b^{k-j}\left(c-\eta R \right) , 
\end{cases}
\end{equation*}
Therefore, 
\begin{equation*}
\sigma_{k+r}(f_\pm)
\le b^{k+r-j}(c+\eta R) = ab^r b^{k-j}\frac{2c}{a+1}
=ab^r b^{k-j}\left(c-\eta R \right) \le a b^r \sigma_{k}(f_\pm),
\end{equation*}
which establishes that $f_\pm \in \calC$.

Although two test functions $f_\pm$ yield the same approximate solution, they have different true solutions.  In particular,
\begin{align*}
\varepsilon &\ge \max \bigl\{\DHJRnorm[\calG]{S(f_+) - A^*(f_+,\varepsilon)}, \DHJRnorm[\calG]{S(f_-) - A^*(f_-,\varepsilon)} \bigr\} \\
&\ge \frac 12 \bigl[\DHJRnorm[\calG]{S(f^*_+) - A^*(f,\varepsilon)} + \DHJRnorm[\calG]{S(f^*_-) - A^*(f,\varepsilon)}  \bigr] \\
&\qquad \qquad \text{since } A^*(f^*_\pm,\varepsilon) = A^*(f,\varepsilon) \\
&\ge \frac 12 \DHJRnorm[\calG]{S(f^*_+) - S(f^*_-)} \quad \text{by the triangle inequality}\\
&\ge \frac 12 \DHJRnorm[\calG]{S(f^*_+ - f^*_-)} \quad \text{since $S$ is linear}\\
&= \eta \DHJRnorm[\calG]{S(u)}.
\end{align*}
Thus, we have
\begin{align*}
\varepsilon^2  & \ge \eta \DHJRnorm[\calG]{S(u)}^2= 
\eta^2 \sum_{k=0}^{j} \DHJRbignorm[\calG]{S(u^{(k)})}^2  \frac{b^{2(k-j)}}{\lambda_{n_k}^2}\\
& \ge \eta^2 
\sum_{k=0}^{j} \DHJRbignorm[\calF]{u^{(k)}}^2 
b^{2(k-j)}\\
& \ge  \eta^2 b^{2(k^{*}-j)} \qquad \text{ where } k^* = \argmax_{0 \le k \le j} \DHJRbignorm[\calF]{u^{(k)}} \\
& \ge \eta^2 = \frac{(a-1)^2c^2}{(a+1)^2R^2} \\
&
=\frac{(a-1)^2 \rho^2}{(a+1)^2A^2}\left[\left(1+\frac{(a-1)^2}{(a+1)^{2} A^2 }\right)\sum_{k=0}^j \frac{b^{2(k-j)}}{\lambda_{n_{k}}^2}\right]^{-1} \\
& \le  \rho^2 \left[\left\{\frac{(a+1)^{2} R^2 }{(a-1)^2} + 1\right\}\sum_{k=0}^j \frac{b^{2(k-j)}}{\lambda_{n_{k}}^2}\right]^{-1} 
\end{align*}

This lower bound must be satisfied by $j$ to be consistent with the assumption $\textup{comp}(\calA(\calC),\varepsilon,\rho) \le n_j - 1$.  Thus, for any $j$ violating this inequality it follows that $\textup{comp}(\calA(\calC),\varepsilon,\rho) \ge n_j$.  This implication provides a lower bound on $\textup{comp}(\calA(\calC),\varepsilon,\rho)$.
\end{proof}

The next step is to show that the cost of our algorithm is essentially no worse than that of the optimal algorithm.

\begin{theorem}
\label{thm:CostNoWorse}
Under assumption \eqref{lambdaRatio} $\textup{cost}(\widetilde{A},\varepsilon,\rho)$ is essentially no worse than $\textup{comp}(\calA(\calC),\varepsilon,\rho)$.
\end{theorem}
\begin{proof}
Let  
\begin{equation} \label{omegadef}
    \omega = \sqrt{\frac{(1 - b^2)}{a^4(1 + b^2R^2 + b^4R^4)}\left[\frac{(a+1)^{2} R^2 }{(a-1)^2} + 1\right]^{-1}},
\end{equation}
and note that it does not depend on $\rho$ or $\varepsilon$ but only on the definition of $\calC$. 
For any positive $\rho$ and $\varepsilon$, Theorem \ref{thm:compcost} says that $\textup{cost}(\widetilde{A},\rho,\varepsilon) \le n_{j^\dagger}$ where 
\begin{align*} 
j^\dagger &\le \min \left \{j \in \bbN : \frac{\rho^2}{\varepsilon^2} \le \frac{(1 - b^2)}{a^2b^2} \left[ \sum_{k=1}^{j-1} \frac{b^{2(k-j)}}{a^2\lambda_{n_{k-1}+1}^2} + \frac{1}{\lambda_{n_{j-1}+1}^2}\right]   \right\} \\
&\le \min \left \{j \in \bbN : \frac{\rho^2}{\varepsilon^2} \le \frac{(1 - b^2)}{a^4b^2} \sum_{k=1}^{j} \frac{b^{2(k-j)}}{\lambda_{n_{k-1}+1}^2} \right\} \qquad \text{since } a > 1\\
&\le \min \left \{j \in \bbN : \frac{\rho^2}{\varepsilon^2} \le \frac{(1 - b^2)}{a^4} \sum_{k=0}^{j-1} \frac{b^{2(k-j)}}{\lambda_{n_{k}+1}^2} \right\} \\
&\le \min \left \{j \in \bbN : \frac{\rho^2}{\varepsilon^2} \le \frac{(1 - b^2)}{a^4} \sum_{k=0}^{j-1} \frac{b^{2(k-j)}}{\lambda_{n_{k}}^2} \right\} \qquad \text{since } \lambda_{n_k} \ge \lambda_{n_k+1}\\
&\le \min \left \{j \in \bbN : \frac{\rho^2}{\varepsilon^2} \le \frac{(1 - b^2)}{a^4(1 + b^2R^2 + b^4R^4)} \sum_{k=0}^{j+1} \frac{b^{2(k-j)}}{\lambda_{n_{k}}^2} \right\} \qquad \text{by \eqref{lambdaRatio} } \\
&\le \min \left \{j \in \bbN : \frac{\rho^2}{\omega^2 \varepsilon^2} \le \left[\frac{(a+1)^{2} R^2 }{(a-1)^2} + 1\right] \sum_{k=0}^{j+1} \frac{b^{2(k-j)}}{\lambda_{n_{k}}^2} \right\} \qquad \text{by \eqref{omegadef} } \\
&= \max \left \{j \in \bbN : \frac{\rho^2}{\omega^2 \varepsilon^2} > \left[\frac{(a+1)^{2} R^2 }{(a-1)^2} + 1\right] \sum_{k=0}^{j} \frac{b^{2(k-j)}}{\lambda_{n_{k}}^2} \right\} =:j^*.
\end{align*}
By Theorem \ref{thm:lowbdcomp}, $\textup{comp}(\calA(\calC),\omega\varepsilon,\rho) \ge n_{j^*}$, and by the argument above, $n_{j^*} \ge n_{j^\dagger} \ge \textup{cost}(\widetilde{A},\varepsilon,\rho)$.  Thus, our algorithm is essentially no more costly than the optimal algorithm.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Numerical Example} \label{sec:examp}

Consider the case of approximating the  partial derivative with respect to $x_1$ of periodic functions defined on the $d$-dimensional unit cube:
\begin{align*}
f &= \sum_{\bsk \in \bbZ^d} \widehat{f}(\bsk) u_{\bsk} (\bsx), \\
u_{\bsk} (\bsx) &:= \prod_{j=1}^d \frac{2^{(1-\delta_{k_j,0})/2}\cos(2\pi k_j x_j  + \bbone_{(-\infty,0)}(k_j) \pi/2 )}{\max(1,\gamma_j k_j)},  \\
S(f) &: = \frac{\partial f}{\partial x_1} = \sum_{\bsk \in \bbZ^d} \widehat{f}(\bsk) \lambda_{\bsk} v_{\bsk} (\bsx), \\
v_{\bsk}(\bsx) & : =  -\textup{sign}(k_1) 2^{(1-\delta_{k_1,0})/2} \sin(2\pi k_j x_j  + \bbone_{(-\infty,0)}(k_j) \pi/2 ) \\
& \qquad \qquad \times \prod_{j=2}^d 2^{(1-\delta_{k_j,0})/2} \cos(2\pi k_j x_j  + \bbone_{(-\infty,0)}(k_j) \pi/2 ), \\
\lambda_{\bsk} &:= \frac{2 \pi \DHJRabs{k_1}}{\prod_{j=1}^d\max(1,\gamma_j k_j)}.
\end{align*}
We construct a function by choosing its Fourier coefficients $\widehat{f}$ to be normal distributed \textbf{What distribution (mean, standard deviation)?  How many Fourier coefficients are chosen to construct the function?}. Let $d=3, a= 2, b=\frac{1}{2},$ and $\varepsilon = 0.1.$ Based on our algorithm, we obtain $ n_{j^\dagger} = 1024.$ Using that, we can obtain the approximated solution and error in Figure \ref{solfig}. Based on Figure \ref{solfig}(b), we can obtain the exact errors are less than $0.01 < \varepsilon $.  This shows our algorithm works.


\begin{figure}[ht]
	\centering
	\begin{tabular}{cc}
		\includegraphics[width =5.5 cm]{ProgramsImages/SimDirectSolAppx.eps} &
		\includegraphics[width = 5.5 cm]{ProgramsImages/SimDirectSolErr.eps}
		\\ (a) & (b)
	\end{tabular}
	\caption{(a) The approximation of the first partial derivative of $f$
		(b) The error of the approximation 
		\label{solfig}} % 
\end{figure}

\begin{figure}[ht]
	\centering
	\begin{tabular}{cc}
		\includegraphics[width = 5.5 cm]{ProgramsImages/SimDirectInpFunAppx.eps}& 
		\includegraphics[width = 5.5 cm]{ProgramsImages/SimDirectInpFunErr.eps}
		\\ (a) & (b)
	\end{tabular}
	\caption{(a) The approximation of the input function $f$
		(b) The error of the approximation 
		\label{inpfig}} % 
\end{figure}

Actually, we can also use these 1024 Fourier coefficients to approximate the input function. The above is the approximation of the input function $f$. Notice that, the errors in Figure \ref{inpfig}(b) are much smaller than the errors in Figure \ref{solfig}(b). 


%\subsection{Examples: Approximating the divergence of a vector field}
%
%In this case, $\calF=\{f:\reals^n\mapsto\reals^m,\, f\in C^1(\Omega)\}$ and $\calG=\{g:\reals^n\mapsto\reals,\, g\in C^0(\Omega)\}$ and $S: f\mapsto \nabla\cdot f=\sum_{j=1}^{m}\partial_{x_j}f_j$.
%
%\begin{gather*}
%\DHJRnorm[\calF]{f} =\left[\int_\Omega \sum_{j=1}^{m}\DHJRabs{f_j(x)}^2+\sum_{j=1}^{m}\DHJRabs{f'_j(x)}^2 \, \dif x \right]^{1/2},\\
%\DHJRnorm[\calG]{g} = \left[\int_\Omega \DHJRabs{g(x)}^2 \, \dif x \right]^{1/2}, \\
%u_{i}(x) = \frac{\me^{2 \pi \sqrt{-1} <i,x>}}{\sqrt{mV(\omega)+4\pi<i,i>}}, \\
%v_i(x) = \me^{2 \pi \sqrt{-1} <i,x>},\\
% \lambda_i=\frac{2\pi\sqrt{-1}\sum_{j=1}^{m}i_j}{\sqrt{mV(\omega)+4\pi<i,i>}}, \qquad i \in \bbZ^m,\\
%\pin=\pout=2,\quad q=\infty.
%\end{gather*}

\section{Discussion and Conclusions} \label{sec:conc}

%\section*{References}
%\nocite{*}
\bibliographystyle{elsarticle-num}
\bibliography{FJH23,FJHown23}


\end{document}






\begin{remark}
For the algorithm, $\widetilde{A}$, defined in Algorithm \ref{algo2}, $\textup{cost}(\widetilde{A},\rho,\varepsilon) \le n_{j^*}$, where $j^*$ satisfies the following upper bound:
\[
j^* \le \min \left \{j \in \bbN : \rho^2 \le \frac{\varepsilon^2(1 - b^2)}{a^2b^2C_{\textup{up}}^2n_0^{2(1-p)}} \left[ \frac{1-b^2}{a^2b^{2(j-1)}}+ \frac{1}{2^{2(j-1)(1-p)}}\right]\right\}.
\]
\end{remark}


\begin{proof}
First, we have a rough upper bound on $\textup{cost}(\widetilde{A},\rho,\varepsilon)$ in the proof of  Theorem  \ref{thm:compcost} by finding the smallest $j$ violating the following inequality:
\begin{equation*}
\rho \ge \DHJRnorm[\calF]{f} \ge \DHJRbignorm[2]{\big( \widehat{f}_i \big)_{i=n_{j-1}+1}^{n_j}} \ge \frac{\sigma_j(f)}{\lambda_{n_{j-1}+1}} > \frac{\varepsilon\sqrt{1 - b^2}}{ab \lambda_{n_{j-1}+1}}
\end{equation*}
If we consider $\lambda_{i}$ is bounded above
\begin{equation} \label{lambdaDecI}
\lambda_i \le C_{\textup{up}} i^{1-p}, \quad   i \in \bbN,
\end{equation}
where $p > 1$, we obtain
\begin{align*}
\rho & > \frac{\varepsilon\sqrt{1 - b^2}}{ab \lambda_{n_{j-1}+1}} \\
& \ge \frac{\varepsilon\sqrt{1 - b^2}}{ab C_{\textup{up}}(n_{j-1}+1)^{1-p}} \qquad \text{by \eqref{lambdaDecI}} \\
& > \frac{\varepsilon\sqrt{1 - b^2}}{ab C_{\textup{up}}n_{j-1}^{1-p}} \\
& \ge \frac{\varepsilon\sqrt{1 - b^2}}{ab C_{\textup{up}}(2^{j-1}n_0)^{1-p}} \qquad \text{by \eqref{geonj}} \\
& = \frac{\varepsilon\sqrt{1 - b^2}}{ab C_{\textup{up}}n_0^{1-p} 2^{(j-1)(1-p)}}
\end{align*}
Thus, one upper bound on $j^*$ is the smallest $j$ violating the above inequality:
\[
j^* \le \min \left \{j \in \bbN :  
j \ge \frac{\log\left[\frac{\varepsilon \sqrt{1-b^2}}{\rho ab C_{\textup{up}}n_0^{1-p}}\right]}{(1-p)\log 2}+ 1\right\}
\]

We could also obtain another upper bound similar to the tighter upper bound in Theorem \ref{thm:compcost}.  
The tighter cost is the smallest $j$ that violates the following inequality
\begin{equation*}
\rho^2 > \frac{\varepsilon^2(1 - b^2)}{a^2b^2} \left[ \sum_{k=1}^{j-1} \frac{ b^{2(k-j)}}{a^{2}\lambda_{n_{k-1}+1}^2} + \frac{1}{\lambda_{n_{j-1}+1}^2}\right].
\end{equation*}
From \eqref{lambdaDecI}, we can obtain for any $k=1, \ldots,j-1$
\[\lambda_{n_{k-1}+1} \le C_{\textup{up}}(n_{k-1}+1)^{1-p} 
<  C_{\textup{up}}(n_{k-1})^{1-p} < C_{\textup{up}}n_0^{1-p}.\]
Thus, we have 
\begin{align*}
\rho^2 &> \frac{\varepsilon^2(1 - b^2)}{a^2b^2} \left[ \sum_{k=1}^{j-1} \frac{ b^{2(k-j)}}{a^{2}\lambda_{n_{k-1}+1}^2} + \frac{1}{\lambda_{n_{j-1}+1}^2}\right]\\
& >  \frac{\varepsilon^2(1 - b^2)}{a^2b^2} \left[ \sum_{k=1}^{j-1} \frac{ b^{2(k-j)}}{a^{2}(C_{\textup{up}}n_0^{1-p})^2} + \frac{1}{(C_{\textup{up}}(n_{j-1})^{1-p} )^2}\right] \\
& = \frac{\varepsilon^2(1 - b^2)}{a^2b^2} \left[ \sum_{k=1}^{j-1} \frac{ b^{2(k-j)}}{a^{2}C_{\textup{up}}^2n_0^{2(1-p)}} + \frac{1}{C^2_{\textup{up}}(2^{j-1}n_0)^{2(1-p)}}\right] \qquad \text{by \eqref{geonj}}\\
&  =\frac{\varepsilon^2(1 - b^2)}{a^2b^2C_{\textup{up}}^2n_0^{2(1-p)}} \left[ \frac{1}{a^2b^{2j}}\sum_{k=1}^{j-1} b^{2k }+ \frac{1}{2^{2(j-1)(1-p)}}\right] \\
& > \frac{\varepsilon^2(1 - b^2)}{a^2b^2C_{\textup{up}}^2n_0^{2(1-p)}} \left[ \frac{1-b^2}{a^2b^{2(j-1)}}+ \frac{1}{2^{2(j-1)(1-p)}}\right] .
\end{align*}
Hence, any $j$ that violates the above inequality, must satisfy $j \ge j^*$.
\end{proof}




/fi
\iffalse
\subsection{The embedding of $f(x)=\frac{(b^2-1)}{b^2+1-2b\cos(2\pi x)}$}

In our example, consider $\calF=\calG=\cl_2[0,1]$, and $S: f \mapsto f$ the embedding operator with:
\begin{gather*}
\DHJRnorm[\calF]{f} = \DHJRnorm[\calG]{f} = \left[\int_0^1 \DHJRabs{f(x)}^2 \, \dif x \right]^{1/2}, \\
u_{i}(x) = v_i(x) = \me^{2 \pi \sqrt{-1} i x},\quad \lambda_i=1, \qquad i \in \bbZ,\\
\pin=\pout=2,\quad q=\infty.
\end{gather*}

For our index set $\ci=\bbZ=\{0,1,-1,2,-2,\ldots\}$,
\begin{gather*}
i_j=(-1)^{j}\left\lfloor\frac{j}{2}\right\rfloor,\qquad j\in\bbN,\\
n_0=0,\quad n_k=2k+1, \qquad k\in\bbN.
\end{gather*}

The Fourier coefficients of this function can be easily found:
\begin{align*}
f(x)&=\frac{b^2-1}{b^2+1-2b\cos(2\pi x)}\\
&=\frac{1-b^{-2}+b^{-1}(\me^{-2\pi\sqrt{-1}x}-\me^{-2\pi\sqrt{-1}x})}{1+b^{-2}-b^{-1}(\me^{2\pi\sqrt{-1}x}+\me^{-2\pi\sqrt{-1}x})}\\
&=\frac{(1-b^{-1}\me^{-2\pi\sqrt{-1}x})+(1-b^{-1}\me^{2\pi\sqrt{-1}x})b^{-1}\me^{-2\pi\sqrt{-1}x}}{(1-b^{-1}\me^{2\pi\sqrt{-1}x})(1-b^{-1}\me^{-2\pi\sqrt{-1}x})}\\
&=\frac{1}{1-b^{-1}\me^{2\pi\sqrt{-1}x}}+\frac{b^{-1}\me^{-2\pi\sqrt{-1}x}}{1-b^{-1}\me^{-2\pi\sqrt{-1}x}}\\
&=\sum_{k\in\bbN_0}\left(b^{-1}\me^{2\pi\sqrt{-1}x}\right)^k+\sum_{k\in\bbN}\left(b^{-1}\me^{-2\pi\sqrt{-1}x}\right)^k\\
&=\sum_{k\in\bbZ}b^{-\DHJRabs{k}}\me^{2\pi\sqrt{-1}kx}\Longrightarrow\widehat{f}_{i_j}=b^{-\DHJRabs{i_j}}
\end{align*}

Because we are interested in seeing how far the approximation is from the real solution, the norm of the exact solution has to be found. Knowing the Fourier coefficients leads us to an easy way to calculate it:
\begin{align*}
\DHJRnorm[\calG]{S(f)}&=\left(\sum_{j\in\bbZ}\lambda_{i_j}^2\widehat{f}_{i_j}^2\right)^{\frac{1}{2}} =\left(1+2\sum_{k\in\bbN}b^{-2k}\right)^{\frac{1}{2}}=\left(\frac{b^2+1}{b^2-1}\right)^{\frac{1}{2}}
\end{align*}

We can also give the sums of coefficients,
\begin{align*}
&\sigma_1(f)=\sqrt{1+2b^{-2}}\\
&\sigma_k(f)=\sqrt{2}b^{-k}, \qquad k \in\bbN\setminus\{1\}.
\end{align*}
Recalling the alternative definition of the cone in (\ref{decayconedef}), with these sums, $f\in\calC$ for $\gamma_r=b^{-r}$ since
\begin{equation*}
\sigma_k(f) \le \min_{1 \le i <k}\{\gamma_i\sigma_{k-i}(f)\}=\min\left\{1 \; , \; \sqrt{1+\frac{b^2}{2}} \right\}\sigma_k(f)
\end{equation*}

In this case, $\DHJRnorm[\pout]{\bgamma}=\frac{1}{\sqrt{b^2-1}}$. If we use our Algorithm \ref{algo2}, we have to check first whether $\sqrt{\frac{b^2+2}{b^4-b^2}}\le\varepsilon$. If this inequality states, then $k^*=1$. If not,
\begin{equation*}
k^*=\min\left\{k\in\bbN\setminus\{1\}:\frac{1}{2\log(b)}\log\left(\frac{2}{(b^2-1)\varepsilon^2}\right) \le k \right\}
\end{equation*}

For a numerical application, set $b=3$ and find below a table of results for this example,

\begin{center}
\begin{tabular}{|c|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  $\varepsilon$ & $k^*$ & $n_{k^*}$ & $\operatorname{error}=\DHJRnorm[\calG]{S(f)}-\DHJRnorm[\calG]{A_{n_{k^*}}(f)}$ \\
  \hline
  $1$ & $1$ & $3$ & $1.2492\cdot10^{-2}$ \\
  $10^{-1}$ & $2$ & $5$ & $1.3811\cdot10^{-3}$ \\
  $10^{-2}$ & $4$ & $9$ & $1.7041\cdot10^{-5}$ \\
  $10^{-3}$ & $6$ & $13$ & $2.1038\cdot10^{-7}$ \\
  $10^{-4}$ & $8$ & $17$ & $2.5973\cdot10^{-9}$ \\
  $10^{-5}$ & $10$ & $21$ & $3.2065\cdot10^{-11}$ \\
  $10^{-6}$ & $12$ & $25$ & $3.9591\cdot10^{-13}$ \\
  $10^{-7}$ & $15$ & $31$ & $4.4409\cdot10^{-16}$ \\
  \hline
\end{tabular}
\end{center}

\subsection{Derivation of the Bernoulli polynomials}

The properties we will use for these particular polynomials can be found in \cite{AbrSte64}: derivatives, explicit Fourier coefficients, etc. To deal with this example, now $\calF=\calG=\cl_2[0,1]$, and $S: f \mapsto f'$ with,
\begin{gather*}
\DHJRnorm[\calF]{f} = \left[\int_0^1 \left\{\DHJRabs{f(x)}^2 + \DHJRabs{f'(x)}^2\right\}\, \dif x\right]^{1/2}, \quad \DHJRnorm[\calG]{f} = \left[\int_0^1 \DHJRabs{f(x)}^2 \, \dif x \right]^{1/2}, \\
u_{i}(x) = \frac{\me^{2 \pi \sqrt{-1} i x}}{\sqrt{1+4 \pi^2 i^2}}, \quad v_i(x) = \me^{2 \pi \sqrt{-1} i x}, \quad \lambda_i=\frac{2 \pi \sqrt{-1} i}{\sqrt{1+4 \pi^2 i^2}}, \qquad i \in \bbZ,\\
\pin=\pout=2,\quad q=\infty.
\end{gather*}

Like in the previous example, the index set $\ci=\bbZ=\{0,1,-1,2,-2,\ldots\}$ has the following ordering
\begin{gather*}
i_j=(-1)^{j}\left\lfloor\frac{j}{2}\right\rfloor,\qquad j\in\bbN,\\
n_0=0,\quad n_k=2^k, \qquad k\in\bbN.
\end{gather*}

For this case,
\begin{equation*}
B_n(x)=-\frac{n!}{(2\pi\sqrt{-1})^n}\sum_{k\in\bbZ\setminus\{0\}}\frac{\me^{2\pi\sqrt{-1}kx}}{k^n}\Longrightarrow
\left\{\begin{array}{l}
\widehat{B_n}_0=0 \\
\widehat{B_n}_{i_j}=-\frac{n!\sqrt{1+4 \pi^2 i_j^2}}{(2\pi\sqrt{-1}i_j)^n},\quad i_j\in\bbZ\setminus\{0\}
\end{array}\right.
\end{equation*}

Using that $B'_n(x)=nB_{n-1}(x)$ and $\int_0^1B_n(t)B_m(t)\dif t=(-1)^{n-1}\frac{m!n!}{(m+n)!}B_{n+m}$, the real solution comes automatically,
\begin{equation*}
\DHJRnorm[\calG]{S(B_n(x))}=n\DHJRnorm[\calG]{B_{n-1}(x)}=n!\sqrt{\frac{\DHJRabs{B_{2(n-1)}}}{[2(n-1)]!}}
\end{equation*}
where $B_n=B_n(0)$ are the Bernoulli numbers.

If we consider $\gamma_r=s_1s_2^{-r}$ and that
\begin{equation}\label{bound}
\frac{n!}{\sqrt{2}\pi^n}j^{-n} \le \DHJRabs{\lambda_{i_j}\widehat{B_n}_{i_j}} \le \frac{n!2^n}{\pi^n}j^{-n},\quad j\in\bbN\setminus\{1\}
\end{equation}
then $C_{\textup{up}}=2^{n+\frac{1}{2}}C_{\textup{lo}}=\frac{n!2^n}{\pi^n}$. Thus, as shown in the example example at the beginning of this section we can take
\begin{equation*}
s_1=2\times11^{n-\frac{1}{2}},\quad s_2=2^{n-\frac{1}{2}}
\end{equation*}
There is still one thing to check because $j=1$ was not taken into account in the inequality (\ref{bound}). However, see that $\sigma_1(f) \ge C_{\textup{lo}}\left(\frac{5}{36}\right)^{n-\frac{1}{2}}$ what means that the $\gamma_r=s_1s_2^{-r}$ found satisfies our needs.

For a numerical application, find below the results for the case $n=5$ and $n=10$,

\begin{center}
\begin{tabular}{|c|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  \multicolumn{4}{|c|}{$B_5(x)=x^5-\frac{5}{2}x^4+\frac{5}{3}x^3-\frac{1}{6}x$} \\
  \hline
  $\varepsilon$ & $k^*$ & $n_{k^*}$ & $\operatorname{error}=\DHJRnorm[\calG]{S(B_5(x))}-\DHJRnorm[\calG]{A_{n_{k^*}}(B_5(x))}$ \\
  \hline
  $1$ & $5$ & $32$ & $2.9439\cdot10^{-11}$ \\
  $10^{-1}$ & $6$ & $64$ & $2.2703\cdot10^{-13}$ \\
  $10^{-2}$ & $7$ & $128$ & $1.8735\cdot10^{-15}$ \\
  $10^{-3}$ & $7$ & $128$ & $1.8735\cdot10^{-15}$ \\
  $10^{-4}$ & $8$ & $256$ & $1.1102\cdot10^{-16}$ \\
  $10^{-5}$ & $9$ & $512$ & $9.7145\cdot10^{-17}$ \\
  \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{|c|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  \multicolumn{4}{|c|}{$B_{10}(x)=x^{10}-5x^9+\frac{15}{2}x^8-7x^6+5x^4-\frac{3}{2}x^2+\frac{5}{66}$} \\
  \hline
  $\varepsilon$ & $k^*$ & $n_{k^*}$ & $\operatorname{error}=\DHJRnorm[\calG]{S(B_{10}(x))}-\DHJRnorm[\calG]{A_{n_{k^*}}(B_{10}(x))}$ \\
  \hline
  $1$ & $5$ & $32$ & $7.7716\cdot10^{-16}$ \\
  $10^{-1}$ & $5$ & $32$ & $7.7716\cdot10^{-16}$ \\
  $10^{-2}$ & $6$ & $64$ & $7.7716\cdot10^{-16}$ \\
  $10^{-3}$ & $6$ & $64$ & $7.7716\cdot10^{-16}$ \\
  $10^{-4}$ & $7$ & $128$ & $7.7716\cdot10^{-16}$ \\
  $10^{-5}$ & $7$ & $128$ & $7.7716\cdot10^{-16}$ \\
  \hline
\end{tabular}
\end{center}

\begin{remark}
MATLAB's machine epsilon is $2.2204\cdot10^{-16}$.
\end{remark}
\fi




\iffalse

We do not expect the series coefficients of the solution $S(f)$, to decay monotonically.  The sequence $\bsn$ is used to average finite sequences of Fourier coefficients into groups to smooth the peaks we could encounter if we take these coefficients one by one. Flattening the peaks is a must to fit functions into the cone. Nevertheless, the solution of our algorithm is in this set $\cn$ which means that the sequence does not have to distance the optimal number of data needed in our algorithm (defined below) more than a constant.

We define the optimal number of data needed to solve the problem defined as \eqref{errCond} is 
\begin{equation*}
N_{\opt}(f,\varepsilon)=\min\left\{n\in\bbN:\,\DHJRnorm[2]{\left(\lambda_{i_j} \widehat{f}_{i_j} \right)_{j\ge n+1}} \le \varepsilon \right\}
\end{equation*}

Any algorithm is called \emph{asymptotically  optimal} if it essentially tracks the optimal number of data needed,
\begin{equation} \label{nearoptdef}
\sup_{\varepsilon > 0 } \frac{\textup{cost}(A,f;\varepsilon)} {N_{\opt}(f,\varepsilon)} <\infty 
\end{equation}
or
\[
\sup_{\varepsilon > 0 } [\textup{cost}(A,f;\varepsilon)- N_{\opt}(f,\varepsilon)] <\infty.
\]

Next, we prove our algorithm is optimal.

\begin{theorem}\label{nopt}
Algorithm \ref{algo2} is optimal for this problem.

\end{theorem}
\begin{proof}
Given $f \in \calC$ and $\varepsilon$, denote $n^*= N_{\opt}(f,\varepsilon)$. Let 
\[n_{k^*-r-1} < n^{*}  \le n_{k^*-r} \le n_{k^*},\]
where $n_{k^*}$ is the cost of our algorithm.
Thus by \eqref{sumdef}, we obtain
\[\sigma_{k^*-r+1}(f) = \DHJRnorm[2]{ \left(\lambda_{i_j} \widehat{f}_{i_j} \right)_{j=n_{k^*-r}+1}^{n_{k^*-r+1}}}
 \le \DHJRnorm[2]{ \left(\lambda_{i_j} \widehat{f}_{i_j} \right)_{j=n^*}^{\infty}} \le \varepsilon.\]
By the cone condition \eqref{decayconedef} , we have 
\[\sigma_{k^*-r+m}(f) \le \gamma_{m-1}\sigma_{k^*-r+1}(f) \le\gamma_{m-1} \cdot \varepsilon.\]
Choose $m$ such that $\gamma_{m-1} \le \frac{1}{\DHJRnorm[2]{\vgamma}},$
thus we obtain
\[\sigma_{k^*-r+m}(f) \le\gamma_{m-1} \cdot \varepsilon \le \frac{\varepsilon}{\DHJRnorm[2]{\vgamma}}.\]
Suppose $\gamma_r$ decays geometrically, $n_k$ increases geometrically $n_k = n_0 2^k $, we have
\[\frac{\textup{cost}(\widetilde{A},f;\varepsilon)}{N_{\opt}(f,\varepsilon)} \le 
\frac{n_{k^*-r+m}}{n_{k^*-r-1}} = \frac{n_0 2^{k^*-r+m}}{n_0 2^{k^*-r-1}}
= 2^{m+1}.\]


As $m$ satisfied 
\begin{align*}\
\gamma_{m-1} \le \frac{1}{\DHJRnorm[2]{\vgamma}} 
& \Longleftrightarrow C_1C_2^{-m+1} \le \frac{\sqrt{C_2^2-1}}{C_1} \\
& \Longrightarrow C_2^{-m+1} \le \frac{\sqrt{C_2^2-1}}{C^2_1} \\
& \Longrightarrow  m \ge 1+ 
\frac{\log\left(\frac{C_1^2}{\sqrt{C_2^2-1}}\right)}{\log(C_2)}.
\end{align*}
Therefore we can choose 
\[m = \left\lceil 1+ 
\frac{\log\left(\frac{C_1^2}{\sqrt{C_2^2-1}}\right)}{\log(C_2)} \right\rceil
 < 2 + \frac{\log\left(\frac{C_1^2}{\sqrt{C_2^2-1}}\right)}{\log(C_2)}. \]
Hence,
 \[\frac{\textup{cost}(A,f;\varepsilon)} {N_{\opt}(f,\varepsilon)} 
 \le 2^{m+1} < 2^{3+\frac{\log\left(\frac{C_1^2}{\sqrt{C_2^2-1}}\right)}{\log(C_2)}} \Longrightarrow \sup_{f,\varepsilon} \frac{\textup{cost}(A,f;\varepsilon)} {N_{\opt}(f,\varepsilon)} <\infty.\]

Suppose $\gamma_r$ decays geometrically, $n_k$ increases linearly $n_k = n_0 + 2k $, we have
\begin{align*}
\textup{cost}(\widetilde{A},f;\varepsilon)-N_{\opt}(f,\varepsilon) & \le 
n_{k^*-r+m}-n_{k^*-r-1} \\ & = (n_0 +2(k^*-r+m))-(n_0+2(k^*-r-1))\\
& = 2(m+1).
\end{align*}
Still choose
\[m = \left\lceil 1+ 
\frac{\log\left(\frac{C_1^2}{\sqrt{C_2^2-1}}\right)}{\log(C_2)} \right\rceil
 < 2 + \frac{\log\left(\frac{C_1^2}{\sqrt{C_2^2-1}}\right)}{\log(C_2)}. \]
 We have
\begin{align*}
\textup{cost}(\widetilde{A},f;\varepsilon)-N_{\opt}(f,\varepsilon) 
& = 2(m+1) \\
&  = 2\left(3+ \frac{\log\left(\frac{C_1^2}{\sqrt{C_2^2-1}}\right)}{\log(C_2)}\right)\\
\Longrightarrow &
\sup_{f,\varepsilon} \left[\textup{cost}(A,f;\varepsilon) -N_{\opt}(f,\varepsilon) \right]< \infty
\end{align*}
\end{proof}

\fi


